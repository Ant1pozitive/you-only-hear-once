{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "efc506b8",
   "metadata": {},
   "source": [
    "# YOHO Demo Notebook\n",
    "\n",
    "Demonstration of inference and visualization using the `YOHO model`.\n",
    "\n",
    "**Requirements:**\n",
    "- Trained model checkpoint (best_yoho.pth)\n",
    "- ESC-50 style dataset structure (optional)\n",
    "- Required packages: torch, torchaudio, matplotlib, librosa, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "from pathlib import Path\n",
    "\n",
    "# Import model and utilities\n",
    "# Adjust paths according to your project structure\n",
    "from model.yoho import YOHO\n",
    "from config import YOHOConfig\n",
    "from utils.visualize import plot_spectrogram_with_preds\n",
    "\n",
    "print(\"Imports completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad506870",
   "metadata": {},
   "source": [
    "## 1. Configuration and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7477b147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration (use the same as during training)\n",
    "cfg = YOHOConfig()\n",
    "\n",
    "# Override some parameters if needed\n",
    "cfg.model.num_classes = 50  # ESC-50 has 50 classes\n",
    "cfg.model.seg_enabled = True\n",
    "cfg.model.use_memory = False  # disable memory for simple demo\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load model\n",
    "model = YOHO(cfg.model).to(device)\n",
    "model.eval()\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint_path = Path(\"checkpoints/best_yoho.pth\")  # <- change to your actual path\n",
    "if checkpoint_path.exists():\n",
    "    model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "    print(f\"Model loaded successfully from {checkpoint_path}\")\n",
    "else:\n",
    "    print(f\"Checkpoint not found at {checkpoint_path}\")\n",
    "    print(\"Demo will run with random weights (predictions will be meaningless)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043c2f7d",
   "metadata": {},
   "source": [
    "## 2. Generate or Load Sample Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d75a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_audio(duration_sec=5.0, sr=44100):\n",
    "    \"\"\"\n",
    "    Generate simple synthetic audio: sine wave + noise + second tone\n",
    "    \"\"\"\n",
    "    t = torch.linspace(0, duration_sec, int(sr * duration_sec))\n",
    "    \n",
    "    # 440 Hz tone (A4 note)\n",
    "    tone1 = 0.7 * torch.sin(2 * np.pi * 440 * t)\n",
    "    \n",
    "    # 880 Hz tone starting at 2 seconds\n",
    "    tone2 = 0.5 * torch.sin(2 * np.pi * 880 * (t - 2.0)) * (t >= 2.0)\n",
    "    \n",
    "    # Background noise\n",
    "    noise = 0.15 * torch.randn_like(t)\n",
    "    \n",
    "    waveform = tone1 + tone2 + noise\n",
    "    \n",
    "    # Normalize\n",
    "    waveform = waveform / (torch.max(torch.abs(waveform)) + 1e-8)\n",
    "    \n",
    "    return waveform, sr\n",
    "\n",
    "\n",
    "# Option 1: Synthetic audio\n",
    "waveform, sr = generate_synthetic_audio(duration_sec=5.0)\n",
    "\n",
    "# Option 2: Load real file (uncomment if you have sample)\n",
    "# waveform, sr = torchaudio.load(\"data/sample.wav\")\n",
    "# waveform = waveform.mean(0)  # to mono\n",
    "\n",
    "print(f\"Audio shape: {waveform.shape}, sample rate: {sr} Hz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09e4ea6",
   "metadata": {},
   "source": [
    "## 3. Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6e8ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input (add batch dimension)\n",
    "audio_input = waveform.unsqueeze(0).to(device)  # [1, T]\n",
    "\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    predictions = model.infer(audio_input, conf_thres=0.25, iou_thres=0.45)\n",
    "\n",
    "print(\"Inference completed\")\n",
    "print(f\"Detected events: {len(predictions.get('boxes', []))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nStreaming inference example:\")\n",
    "long_audio = torch.cat([waveform] * 10)  # 50 seconds\n",
    "stream_preds = model.stream_infer(long_audio, chunk_length_sec=5.0, overlap_sec=1.5)\n",
    "print(f\"Processed {len(stream_preds)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c58362",
   "metadata": {},
   "source": [
    "## 4. Compute Spectrogram for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afdb697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same parameters as in model\n",
    "n_mels = cfg.model.spec.n_mels if hasattr(cfg.model, 'spec') else 128\n",
    "hop_length = cfg.model.spec.hop_length if hasattr(cfg.model, 'spec') else 512\n",
    "\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=sr,\n",
    "    n_mels=n_mels,\n",
    "    hop_length=hop_length,\n",
    "    f_max=8000,\n",
    "    normalized=True\n",
    ")\n",
    "\n",
    "spec_tensor = mel_transform(waveform)\n",
    "spec_db = librosa.power_to_db(spec_tensor.numpy(), ref=np.max)\n",
    "\n",
    "print(f\"Spectrogram shape: {spec_db.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73613e3",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7df61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare predictions in format expected by visualize function\n",
    "# Note: adjust according to your actual output structure of .infer()\n",
    "vis_preds = {\n",
    "    'boxes': predictions.get('boxes', torch.empty((0,4))),\n",
    "    'scores': predictions.get('scores', torch.empty((0,))),\n",
    "    'labels': predictions.get('labels', torch.empty((0,), dtype=torch.long)),\n",
    "    'masks': predictions.get('masks', None)\n",
    "}\n",
    "\n",
    "# Plot\n",
    "fig = plot_spectrogram_with_preds(\n",
    "    spec_db,\n",
    "    vis_preds,\n",
    "    title=\"YOHO Inference on Synthetic Audio\",\n",
    "    save_path=\"demo_result.png\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a1874e",
   "metadata": {},
   "source": [
    "## 6. Optional: Detailed Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d05c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(vis_preds['boxes']) > 0:\n",
    "    print(\"\\nDetected events:\")\n",
    "    for i, (box, score, label) in enumerate(zip(\n",
    "        vis_preds['boxes'],\n",
    "        vis_preds['scores'],\n",
    "        vis_preds['labels']\n",
    "    )):\n",
    "        t_start, f_start, t_end, f_end = box.tolist()\n",
    "        print(f\"Event #{i+1}:\")\n",
    "        print(f\"  Time: {t_start:.2f}s â†’ {t_end:.2f}s\")\n",
    "        print(f\"  Frequency: {f_start:.0f}Hz â†’ {f_end:.0f}Hz\")\n",
    "        print(f\"  Class: {label.item()}  |  Confidence: {score.item():.3f}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No events detected above confidence threshold.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c00fd6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "You can now:\n",
    "- Replace synthetic audio with real recordings\n",
    "- Try different confidence thresholds\n",
    "- Compare results before/after model improvements\n",
    "\n",
    "Happy experimenting! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "construction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
